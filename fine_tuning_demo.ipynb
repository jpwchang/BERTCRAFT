{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5JqrpJWoNaB"
   },
   "source": [
    "# CRAFT fine-tuning and inference interactive demo\n",
    "\n",
    "This example notebook shows how to fine-tune a pretrained CRAFT conversational model for the task of forecasting conversational derailment, as shown in the \"Trouble on the Horizon\" paper (note however that due to nondeterminism in the training process, the results will not exactly reproduce the ones shown in the paper; if you need the exact inference results from the paper, see our [online demo](https://colab.research.google.com/drive/1GvICZN0VwZQSWw3pJaEVY-EQGoO-L5lH) that does inference only using the saved already-fine-tuned model from the paper).\n",
    "\n",
    "Also note that this notebook is written primarily for the Wikipedia data. It will still work on the Reddit CMV data as well, but be aware that if seeking to compare results to those in the paper, the actual Reddit CMV evaluation contains some nuances that are not present in the Wikipedia data, as detailed in the [CMV version of the online demo](https://colab.research.google.com/drive/1aGBUBeiF3jT-GtBU9SDUoxhsjwKZaMKl?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set before importing pytorch: https://discuss.pytorch.org/t/cuda-visible-device-is-of-no-use/10018/12.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "RHkNojY7tzAh",
    "outputId": "a9d10e05-c6db-401f-a651-20bb60a3c095"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/reef/conda-envs/jacq-zissou-env-3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries, including convokit\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import unicodedata\n",
    "import itertools\n",
    "from convokit import download, Corpus\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "from config import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_w8yFiXuCpg"
   },
   "source": [
    "## Part 1: set up data preprocessing utilities\n",
    "\n",
    "We begin by setting up some helper functions for preprocessing the ConvoKit Utterance data for use with CRAFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0SazSyux7KFS"
   },
   "outputs": [],
   "source": [
    "# Given a ConvoKit conversation, preprocess each utterance's text by tokenizing and truncating.\n",
    "# Returns the processed dialog entry where text has been replaced with a list of\n",
    "# tokens, each no longer than MAX_LENGTH - 1 (to leave space for the EOS token)\n",
    "def processDialog(dialog):\n",
    "    processed = []\n",
    "    for utterance in dialog.iter_utterances():\n",
    "        # skip the section header, which does not contain conversational content\n",
    "        if corpus_name == 'wikiconv' and utterance.meta['is_section_header']:\n",
    "            continue\n",
    "        processed.append({\"text\": utterance.text, \"is_attack\": int(utterance.meta[utt_label_metadata]) if utt_label_metadata is not None else 0, \"id\": utterance.id})\n",
    "    if utt_label_metadata is None:\n",
    "        # if the dataset does not come with utterance-level labels, we assume that (as in the case of CMV)\n",
    "        # the only labels are conversation-level and that the actual toxic comment was not included in the\n",
    "        # data. In that case, we must add a dummy comment containing no actual text, to get CRAFT to run on \n",
    "        # the context preceding the dummy (that is, the full prefix before the removed comment)\n",
    "        processed.append({\"text\": \"\", \"is_attack\": int(dialog.meta[label_metadata]), \"id\": processed[-1][\"id\"] + \"_dummyreply\"})\n",
    "    return processed\n",
    "\n",
    "# Load context-reply pairs from the Corpus, optionally filtering to only conversations\n",
    "# from the specified split (train, val, or test).\n",
    "# Each conversation, which has N comments (not including the section header) will\n",
    "# get converted into N-1 comment-reply pairs, one pair for each reply \n",
    "# (the first comment does not reply to anything).\n",
    "# Each comment-reply pair is a tuple consisting of the conversational context\n",
    "# (that is, all comments prior to the reply), the reply itself, the label (that\n",
    "# is, whether the reply contained a derailment event), and the comment ID of the\n",
    "# last comment in the context (for later use in re-joining with the ConvoKit corpus).\n",
    "# The function returns a list of such pairs.\n",
    "def corpus2dataset(corpus, split=None, last_only=False, shuffle=False):\n",
    "    dataset_dict = {\n",
    "        \"context\": [],\n",
    "        \"id\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    for convo in corpus.iter_conversations():\n",
    "        # consider only conversations in the specified split of the data\n",
    "        if split is None or convo.meta['split'] == split:\n",
    "            dialog = processDialog(convo)\n",
    "            iter_range = range(1, len(dialog)) if not last_only else [len(dialog)-1]\n",
    "            for idx in iter_range:\n",
    "                label = dialog[idx][\"is_attack\"]\n",
    "                # when re-joining with the corpus we want to store forecasts in\n",
    "                # the last comment of each context (i.e. the comment directly\n",
    "                # preceding the reply), so we must save that comment ID.\n",
    "                comment_id = dialog[idx-1][\"id\"]\n",
    "                # gather as context all utterances preceding the reply\n",
    "                context = [u[\"text\"] for u in dialog[:idx]]\n",
    "                dataset_dict[\"context\"].append(context)\n",
    "                dataset_dict[\"id\"].append(comment_id)\n",
    "                dataset_dict[\"labels\"].append(label)\n",
    "    if shuffle:\n",
    "        return Dataset.from_dict(dataset_dict).shuffle(seed=2024)\n",
    "    else:\n",
    "        return Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_ev-7g-xsGQ"
   },
   "source": [
    "## Part 2: load the data\n",
    "\n",
    "Now we load the labeled corpus (Wikiconv or Reddit CMV) from ConvoKit, and run some transformations to prepare it for use with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Y96SXcp4x1yj",
    "outputId": "64557a00-453c-44c0-f334-b5ae83508231"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /reef/convokit_corpora_jpc/conversations-gone-awry-corpus\n"
     ]
    }
   ],
   "source": [
    "if corpus_name == \"wikiconv\":\n",
    "    corpus = Corpus(filename=download(\"conversations-gone-awry-corpus\"))\n",
    "elif corpus_name == \"cmv\":\n",
    "    corpus = Corpus(filename=download(\"conversations-gone-awry-cmv-corpus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "fPySt5a4yLId",
    "outputId": "18931f88-572d-4c39-e5da-df3eca9c078c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30021\n",
      "8069\n",
      "4188\n"
     ]
    }
   ],
   "source": [
    "# let's check some quick stats to verify that the corpus loaded correctly\n",
    "print(len(corpus.get_utterance_ids()))\n",
    "print(len(corpus.get_speaker_ids()))\n",
    "print(len(corpus.get_conversation_ids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "eeNfs0A-yosu",
    "outputId": "61b9f041-d59f-4f5a-a65b-ce9631c02336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'obj_type': 'conversation', '_owner': <convokit.model.corpus.Corpus object at 0x7f80ec3680d0>, '_id': '146743638.12652.12652', 'vectors': [], '_meta': ConvoKitMeta({'page_title': 'User talk:2005', 'page_id': 1003212, 'pair_id': '143890867.11926.11926', 'conversation_has_personal_attack': False, 'verified': True, 'pair_verified': True, 'annotation_year': '2018', 'split': 'train'}), '_utterance_ids': ['146743638.12652.12652', '146743638.12667.12652', '146842219.12874.12874', '146860774.13072.13072'], '_speaker_ids': None, 'tree': None}\n",
      "Utterance(id: '146743638.12652.12652', conversation_id: 146743638.12652.12652, reply-to: None, speaker: Speaker(id: 'Sirex98', vectors: [], meta: ConvoKitMeta({})), timestamp: 1185295934.0, text: '== [WIKI_LINK: WP:COMMONNAME] ==\\n', vectors: [], meta: ConvoKitMeta({'is_section_header': True, 'comment_has_personal_attack': False, 'toxicity': 0, 'parsed': [{'rt': 3, 'toks': [{'tok': '=', 'tag': 'NFP', 'dep': 'punct', 'up': 3, 'dn': []}, {'tok': '=', 'tag': 'LS', 'dep': 'punct', 'up': 3, 'dn': []}, {'tok': '[', 'tag': '-LRB-', 'dep': 'punct', 'up': 3, 'dn': []}, {'tok': 'WIKI_LINK', 'tag': 'JJ', 'dep': 'ROOT', 'dn': [0, 1, 2, 4]}, {'tok': ':', 'tag': ':', 'dep': 'punct', 'up': 3, 'dn': []}]}, {'rt': 0, 'toks': [{'tok': 'WP', 'tag': 'NNP', 'dep': 'ROOT', 'dn': [1, 2, 5]}, {'tok': ':', 'tag': ':', 'dep': 'punct', 'up': 0, 'dn': []}, {'tok': 'COMMONNAME', 'tag': 'NNPS', 'dep': 'appos', 'up': 0, 'dn': [3]}, {'tok': ']', 'tag': '-RRB-', 'dep': 'punct', 'up': 2, 'dn': []}, {'tok': '=', 'tag': 'SYM', 'dep': 'punct', 'up': 5, 'dn': []}, {'tok': '=', 'tag': 'SYM', 'dep': 'punct', 'up': 0, 'dn': [4, 6]}, {'tok': '\\n', 'tag': '', 'dep': '', 'up': 5, 'dn': []}]}]}))\n"
     ]
    }
   ],
   "source": [
    "# Let's also take a look at some example data to see what kinds of information/metadata are available to us\n",
    "print(list(corpus.iter_conversations())[0].__dict__)\n",
    "print(list(corpus.iter_utterances())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the corpus into PyTorch-formatted train, val, and test datasets\n",
    "dataset = DatasetDict({\n",
    "    \"train\": corpus2dataset(corpus, \"train\", last_only=True, shuffle=True), \n",
    "    \"val\": corpus2dataset(corpus, \"val\", last_only=True),\n",
    "    \"val_for_tuning\": corpus2dataset(corpus, \"val\"),\n",
    "    \"test\": corpus2dataset(corpus, \"test\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['context', 'id', 'labels'],\n",
       "        num_rows: 2508\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['context', 'id', 'labels'],\n",
       "        num_rows: 840\n",
       "    })\n",
       "    val_for_tuning: Dataset({\n",
       "        features: ['context', 'id', 'labels'],\n",
       "        num_rows: 4399\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['context', 'id', 'labels'],\n",
       "        num_rows: 4365\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data sizes\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"http://www.politico.com/blogs/bensmith/0910/Two_wars.html\\nAgreed. Above is another article from September 2010 where Obama says that the U.S. is still fighting two wars. Thus, since many govt. officials say that the Iraq War isn't over yet, it shouldn't be considered over in this article either.\", \"Well there's obviously a war ongoing in Iraq, with more than two thousand casualties anually, and the UN's criteria for a major war is at least one thousand deaths annually. But I think this article should do more to differentiate the three different phases of the war. The war against Saddam's Iraq, the civil war with US occupation and the civil war without US occupation. The ongoing conflict seems to fit the last phase (or is starting to). It would solve how most of this article is far too big too. \", 'With 50,000 U.S. troops, tens of thousands more private military forces, and thousands of  casualties annually, it\\'s obvious that the war is still going on. Unfortunately some are still trying to have the article present an official end date to the war when there isn\\'t one.\\n\\nJune 14, 2011:\\nUS military deaths in \\'\\'\\'Iraq war\\'\\'\\' at 4,460 Tuesday, according to Associated Press count\\nThe AP count is four fewer than the Defense Department’s tally, last updated Tuesday at 10 a.m. EDT.\\nhttp://www.washingtonpost.com/national/us-military-deaths-in-iraq-war-at-4460-tuesday-according-to-associated-press-count/2011/06/14/AGncHgUH_story.html\\n\\nJune 14, 2011:\\nUS military deaths in \\'\\'\\'Iraq war\\'\\'\\' at 4460\\nhttp://seattletimes.nwsource.com/html/nationworld/2015315910_apusiraqusdeaths.html\\nhttp://www.newsday.com/news/us-military-deaths-in-iraq-war-at-4-460-1.2956241\\n\\nFebruary 1, 2011:\\n\\'\\'\\'Iraq war\\'\\'\\' casualties rise sharply\\nhttp://www.irishtimes.com/newspaper/breaking/2011/0201/breaking13.html\\n\\nJune 19, 2011\\nGates\\' tenure defined by \\'\\'\\'2 wars that outlast him\\'\\'\\'\\nWhen Gates retires on June 30, there will be nearly 50,000 U.S. troops \\'\\'\\'in Iraq\\'\\'\\' and about 100,000 in Afghanistan.\\nhttp://www.cbsnews.com/stories/2011/06/18/ap/politics/main20072250.shtml\\nhttp://www.straitstimes.com/BreakingNews/World/Story/STIStory_681573.html\\n\\nJune 19, 2011\\nThe resolution gave the administration until Friday to respond to a series of questions on the mission, including the scope of U.S. military activity, the cost of the mission and its impact on other \\'\\'\\'U.S. wars in Iraq and Afghanistan\\'\\'\\'.\\nhttp://www.salon.com/news/libya/?story=/news/feature/2011/06/19/ml_libya_26\\n\\nAugust 19, 2010\\nIraq Withdrawal Signals \\'\\'\\'New Phase, But War is Not Over\\'\\'\\'\\nOn Aug. 31, the Obama administration \\'\\'\\'will change the name of the war\\'\\'\\', known as \"Operation Iraqi Freedom,\" to \"Operation New Dawn.\" \\'\\'\\'But it isn\\'t expected to signal and end to the war itself.\\'\\'\\'\\n\\'\\'\\'\"I don\\'t think anybody declared the end of the war as far as I know. There\\'s still fighting ahead,\" the Pentagon\\'s Morell said\\'\\'\\' today.\\nhttp://abcnews.go.com/Politics/iraq-war-major-milestone/story?id=11437254&page;=2\\n\\nSeptember 3, 2010\\nAP Issues Standards Memo: \\'\\'\\'\\'Combat In Iraq Is Not Over\\'\\'\\'\\'\\nUnless there is balancing language, our content should not refer to the end of combat in Iraq, or the end of U.S. military involvement. Nor should it say flat-out (since we can\\'t predict the future) that the United States is at the end of its military role.\\nhttp://www.huffingtonpost.com/2010/09/03/ap-memo-iraq-war_n_705446.html\\n\\nSeptember 6, 2010\\n\\'Withdrawing\\' from the Iraq war doesn\\'t mean it\\'s over\\n\\n\\'\\'\\'the war isn\\'t even close to over.\\'\\'\\' ... \\'\\'\\'the military is quietly acknowledging as much.\\'\\'\\' ... the Pentagon admits \"nothing will change.\" That isn\\'t a paraphrase — it\\'s a \\'\\'\\'direct quote from the Army\\'s chief spokesman in Iraq.\\'\\'\\' It came just before a Colorado Springs Gazette dispatch quoted another military official saying \"our mission has not changed.\"\\nhttp://seattletimes.nwsource.com/html/opinion/2012804856_sirota07.html\\n\\nSeptember 7, 2010\\n\\'\\'\\'AP Tells Its Reporters to Avoid Saying the Iraq War Is Over\\'\\'\\'\\nmilitary officials quietly concede that the \"Iraq War is over\" spin is just that: spin.\\n\\'\\'\\'The fact is, the Iraq war continues\\'\\'\\'\\nhttp://www.huffingtonpost.com/david-sirota/ap-tells-its-reporters-to_b_707508.html\\n\\nAgain, this Wikipedia article should not falsely present an official end date to the war when none exists.  ', \"In order to say that the war is ongoing you are going to need to provide reliable sources WP:RS sources with the weight WP:weight that equal or supersede that of the President and State Department. Policy memos from the AP or editorials from the huffington post don't rise t that level.  \", \"Formats please stop filling the talk page with those quotes, place them between brackets so that it doesn't ruin the layout (thanks).  \", 'Are you saying there is no war currently in Iraq? There is constant fighting, check out icasualties   ', \"95 percent of the most reliable news sources are stating that the war is still not over, as well as the Pentagon. Just because a president, doesn't matter if he is the US president, says one thing it doesn't mean it is true. As far as I know, the US government called the Korean and Vietnam wars ''police actions'' while the rest of the world called them wars. If we took only what the US government said as truth than it would seem the US has mostly conducted police actions and peackeeping operations around the world and not wars. In any case, most sources confirm the war is still ongoing, and a majority concencuss has been reached on Wikipedia to that effect (excluding only two editors).  \"]\n",
      "['We are saying, along with Gruber, that the hilya is laid out to resemble the shape of a body. Gruber cites as evidence the terms navel, skirt and sleeves used to describe parts of the hilya layout. I was a little troubled to find that she evidently mistranslates \"koltuk\", which means seat, armchair or armpit rather than sleeve (see Johnbod\\'s talk). The other sources cited in hilya (notably Osborne, Derman, both in Google Books) don\\'t make this connection between the layout and a body shape either. All the other elements of the hilya model do not have names that could in any way be construed as names of body parts. Thinking out loud here, we speak of a header, body and foot of a page, without implying representation of a human body, and to me the hilyas really don\\'t look like bodies. Islamic calligraphy is perfectly capable of mimicking a body convincingly when it wants to, so I am a little unconvinced here. Further source research welcome. Are there any other sources asserting that a hilya is graphically designed to look like a man\\'s body? \\'\\'\\'\\'\\'\\'', 'For comparison, apart from Osborn, Derman and Safi, here is a book from Indiana University Press, describing the hilye as a \"geometrical, architectural composition\", in \\'\\'contrast\\'\\' to the figurative depictions common in Christianity. A similar argument is made by Mohamed Zakariya, a contemporary hilye artist, as well as by the author of The Cambridge Companion to Muhammad; there is certainly no hint of a suggestion that the layout is designed to mimic a body, and every affirmation that the hilye avoids visual representation. I think Gruber is overreaching here, and at any rate not representative.   \\'\\'\\'\\'\\'\\'', 'This should be copied to hilya. Obviously any resemblance to a body is highly schematic and purely symbolic. If you read the rest of this page, you will see that I am very resistant to suggestions, frankly based on POV, that Gruber, who is a leading contemporary academic in the field, is making mistakes every couple of lines. This is OR in its purest form.  She references that para to Tim Stanley, the senior Middle East curator at the Victoria & Albert Museum, one of the world\\'s leading collections of Islamic art, & a specialist in calligraphy. His article on the hilye that Gruber gives as \"forthcoming\" seems to remain so, though he has given it as a paper or lecture to various specialist conferences etc. There is hardly enough literature in English for a particular view to be considered unrepresentative - the whole subject of the hilye goes unmentioned in the Yale history of art and similar works as far as I can see. We have a clear citation to a high-quality RS, who cites a still more specialist one, and should leave it at that in the absence of any one specifically disputing Stanley/Gruber\\'s view, not merely failing to say the same thing in a brief page or so on the subject.  I am very clear who is \"over-reaching\" here. I can\\'t see the Cambridge companion where I am. No one is saying the hilye is \"representational\".  ', \"But then we'd have to give six (?) times as much space to the various authors who state that it's a geometric, non-representational composition; and I don't think it's worth spending that amount of words on. ''''''\", 'That it is geometric and non-representational does not contradict Gruber in any way - obviously it is both of those things.  ', 'But her argument is that the elements correspond to parts of the prophet\\'s body (\"the hilye\\'s form was conceived in a corporealizing manner so as to recall semantically the Prophet\\'s presence via a graphic construct\"). Would that not make it representational? At any rate, it\\'s not the tack  the other sources take. (I\\'ll type out the Cambridge Companion for you, if you like.) I just don\\'t think it\\'s worth losing words over one way or the other; if we present both views, then we give it too much weight. \\'\\'\\'\\'\\'\\'']\n",
      "['Mewtwo\\'s new transformation is a Forme not a form. Heres Why\\nA form is something like say Deerling, which changes with the seasons. Or Basculin which has a stripe variation. Or shiny forms. This shows no purpose in stats other than the look. Keldeo can\\'t change at will, which is why his is simply a form difference, which does boost stats, but can\\'t be changed at will.\\nA Forme is a legendaries transformation that \\'\\'\\'can\\'\\'\\' be changed at will. As shown in the video, Mewtwo can change at will. It is not simply a new look or unwilling change.\\nThis is the proper change. Meanwhile Ryu Please dont\\nRevert me to simply add the same thing back\\nRevert to make an edit\\nOr cuss and get overreact.\\nYou keep saying \"that I added\" when you give the reason for adding back. I also remind you that Its not my page as you seem to thank. ', 'This reasoning you are putting forth is original research, which is not allowed on Wikipedia. It might pass on some fanwiki, but unless you can find a reliable source that specifically calls \"Awakened Mewtwo\" a \"Forme\" then it should not be referred to as such on Wikipedia. I\\'ve shown sources (in Japanese) used in the article that refer to it simply as a \"form\" or in Japanese \\'\\'sugata\\'\\', rather than a \"Forme\", which in Japanese is \\'\\'Forumu\\'\\'.\\nAnd even then, a \"Forme\" is still a \"form\". Using the latter terminology is more general and the former is still a silly little piece of game jargon that should only be used as part of the name of the form rather than making it something completely unique amongst forms. \"Giratina has an alternate form called Origin Forme\" is a perfectly valid sentence. There is no need to change \"form\" to \"Forme\" in that sentence.\\nSo, based on these two very valid reasons, the word \"Forme\" should not be used in conjunction with discussing \"Awakened Mewtwo\". Unless it is revealed that \"Awakened Mewtwo\" gets some new unique name in Japan like \"Myūtsū Kakusei Forumu\" then \"Forme\" can be used, and even then it\\'s not necessary to refer to \"Awakened Forme\" as \"an alternate Forme\" because that\\'s just bad English due to the fact that Game Freak decided to go with the French word  instead of the English word .— ', 'Dude, you seriously are overreacting. Its not that big of a deal. It is his Awakened Forme and it doesnt matter. Why are you making such a big deal. You dont know everything. It doesnt bother me if its not there, it will when the games come out. I dont know why it \"giv(es) (you) aggravation\". ', 'First of all, learn how to indent to make threaded replies. Second, it is never referred to as \"Awakened Forme\" so we should not use the word \"Forme\" on Wikipedia. That\\'s it. I don\\'t know why \\'\\'you\\'\\' are so pressed on saying \"form should be written as Forme because it\\'s a Legendary\". If it gets called something with \"Forme\" in the name then we can say \"This is Mewtwo\\'s X Forme\" but not say that \"This is an alternate Forme\".\\nAlso, stop removing the valid and reliably sourced statement regarding the Japanese phrase \"Awakened Mewtwo\\'s form\".— ', \"They are many articles that use Forme. Who says we cant use it? But either way I Dont care. It doesnt bother me. But Dude your source '''does not''' mention Awakened Mewtwo or anything other than talking about the Prologue Special. It does '''not''' mention Awakened Mewtwo's form. It is only Awakened Mewtwo and Mewtwo's Awakened Forme. \", 'We can\\'t use \"Forme\" because \\'\\'\\'no reliable source or official website has referred to it as a \"Forme\" yet\\'\\'\\'. And I can read Japanese so I know that it says 覚醒したミュウツーの姿 (\"Awakened Mewtwo\\'s form\") on the first line and ミュウツーの覚醒した姿 (\"Mewtwo\\'s awakened form\") later on. Stick those into Google Translate or learn Japanese before saying that I\\'m wrong. Yes, the page is primarly about the \"Prologue\" movie. But it does mention the Genesect movie.— ', 'Um, Maybe you should learn Japanese because: 覚醒したミュウツーの姿   ミュウツーの覚醒した姿 means \"Figure which was awakening of appearance of Mewtwo Mewtwo which aroused.\" Not what you said. I translated the page and CTRL F the page and it has NO mention of what you said. ']\n",
      "['wiki_link has \"Avoid linking the names of major geographic features and locations, languages, religions, and common professions.\" Are we seriously catering for a readership which is unfamiliar with Ireland, so that we need to link wiki_link? ', 'wiki_link from wiki_link, \"When referring to places and settlements in the Republic of Ireland in the introduction to articles (and in elements such as info boxes), use [[Republic of Ireland|Ireland]] not [[Ireland]] or [[Republic of Ireland]] \", both are valid forms of MOS. Ireland, the article is about the island. People are of states, the wikilink brings the reader to the state, someone would be from Haiti not Hispaniola. \"Are we seriously catering for a readership which is unfamiliar with Ireland\", well yes, the aim is to cater for many different levels of understanding and knowledge, not assume otherwise. I think from reading your comment you didnt know the difference between the name of the country and the name of the article of the country? ', 'Please link me to a project-wide consensus that \"people are of states\" and that Ireland should be an exception to our project-wide guidance on overlinking. Please show evidence that there is an encyclopedic benefit in pipe-linking the name of a country on every occasion it appears in an article on a novelist. ', 'So people are from landmasses? I think you will find a state or country on every bio. And if you took the time to read the guideline you would know its not everytime. In the lead and infobox (and common sense should indicate the rest). ', 'Common sense, ah yes. To me, common sense would point out that wiki_link has a link to wiki_link, for those curious about the political status of this novelist\\'s birthplace. And wiki_link embodies this project-wide \"common sense\". It\\'s an interesting contention that there should be a link to \"a state or country on every bio\"; can you point me to a discussion where this supposed practice is discussed? Because I know that the opposite is true and that whenever this is discussed, thoughtful and experienced editors agree to avoid this type of overlinking. ']\n",
      "[\"'''This geneology was compiled by the following people: ''Andrea Dominici Battelli, Paolo Bonato, Dott. Francis A. Burkle-Young, del Gettysburg College, Pennsylvania, Dott. Nobile Luigi Gonella, Lucia Lopriore, Dario E. Maria Manfredi, del Centro Studi Malaspiniani di Mulazzo (per i Malaspina), Bruno De Martin (per le casate friulane), Don Carlo Notarbartolo Conte di Priolo dei Duchi di Villarosa (per le dinastie siciliane), Cesare Patrignani, R. Kenneth Sheets, Prof. Dott. Herbert Stoyan, dell'Università di Erlangen, Nicolò Tassoni Estense Marchese di Castelvecchio''.'''\\nJust because you don't like the geneology, it doesn't give you the right to call it unofficial. It won't be official for you anyway - as long it states that Skenderbeg was not an Albanian. '''I've also quoted Paul Rovinski, Leopold Ranke, L. Diefenbach, Teodoro Spanduci, Fatos Lubonja (an Albanian) and Kaplan Resulli 9an Albanian)'''.\", '-', '-', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "# check examples to verify that processing happened correctly\n",
    "for i in range(5):\n",
    "    print(dataset[\"train\"][i][\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=20): 100%|██████████| 2508/2508 [00:01<00:00, 1591.78 examples/s]\n",
      "Map (num_proc=20): 100%|██████████| 840/840 [00:00<00:00, 1288.16 examples/s]\n",
      "Map (num_proc=20): 100%|██████████| 4399/4399 [00:02<00:00, 2147.65 examples/s]\n",
      "Map (num_proc=20): 100%|██████████| 4365/4365 [00:01<00:00, 2775.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenize the dataset so it is usable by huggingface\n",
    "# shamelessly stolen from https://github.com/CornellNLP/calm (thanks tushaar)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google-bert/bert-base-cased\", model_max_length=512, truncation_side=\"left\", padding_side=\"right\"\n",
    ")\n",
    "\n",
    "tokenizer_helper = lambda inst: tokenizer.encode_plus(\n",
    "    text=f\" {tokenizer.sep_token} \".join(inst[\"context\"]), \n",
    "    add_special_tokens=True,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    ")\n",
    "tokenized_dataset = dataset.map(tokenizer_helper, remove_columns=[\"context\"], num_proc=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2508\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['id', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 840\n",
       "    })\n",
       "    val_for_tuning: Dataset({\n",
       "        features: ['id', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4399\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4365\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVO=[CLS] '...'''the military is quietly acknowledging as much.'' '... the Pentagon admits \" nothing will change. \" That isn't a paraphrase — it's a'''direct quote from the Army's chief spokesman in Iraq.'''It came just before a Colorado Springs Gazette dispatch quoted another military official saying \" our mission has not changed. \" http : / / seattletimes. nwsource. com / html / opinion / 2012804856 _ sirota07. html September 7, 2010'''AP Tells Its Reporters to Avoid Saying the Iraq War Is Over'''military officials quietly concede that the \" Iraq War is over \" spin is just that : spin.'''The fact is, the Iraq war continues'''http : / / www. huffingtonpost. com / david - sirota / ap - tells - its - reporters - to _ b _ 707508. html Again, this Wikipedia article should not falsely present an official end date to the war when none exists. [SEP] In order to say that the war is ongoing you are going to need to provide reliable sources WP : RS sources with the weight WP : weight that equal or supersede that of the President and State Department. Policy memos from the AP or editorials from the huffington post don't rise t that level. [SEP] Formats please stop filling the talk page with those quotes, place them between brackets so that it doesn't ruin the layout ( thanks ). [SEP] Are you saying there is no war currently in Iraq? There is constant fighting, check out icasualties [SEP] 95 percent of the most reliable news sources are stating that the war is still not over, as well as the Pentagon. Just because a president, doesn't matter if he is the US president, says one thing it doesn't mean it is true. As far as I know, the US government called the Korean and Vietnam wars'' police actions'' while the rest of the world called them wars. If we took only what the US government said as truth than it would seem the US has mostly conducted police actions and peackeeping operations around the world and not wars. In any case, most sources confirm the war is still ongoing, and a majority concencuss has been reached on Wikipedia to that effect ( excluding only two editors ). [SEP]\n",
      "\n",
      "LABEL=0\n"
     ]
    }
   ],
   "source": [
    "# Check to see if everything looks alright.\n",
    "print(\n",
    "    f\"CONVO={tokenizer.decode(tokenized_dataset['train'][0]['input_ids'])}\"\n",
    "    f\"\\n\\nLABEL={tokenized_dataset['train'][0]['labels']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghTVeRFK0CUe"
   },
   "source": [
    "## Part 3: define the inference pipeline\n",
    "\n",
    "CRAFT inference consists of three steps: (1) using the utterance encoder to produce embeddings of each comment in the context (2) running the comment embeddings through the context encoder to get a final representation of conversational context (3) running the classifier head on the context embedding. To streamline the subsequent code, we encapsulate these three steps in a single PyTorch `nn.Module`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjC1hgIWGl7g"
   },
   "source": [
    "## Part 4: define training loop\n",
    "\n",
    "Now that we have all the model components defined, we need to define the actual training procedure. This will be a fairly standard neural network training loop, iterating over batches of labeled dialogs and computing cross-entropy loss on the predicted label. We will also define evaluation functions so that we can compute accuracy on the validation set after every epoch, allowing us to keep the model with the best validation performance. Note that for the sake of simpler code, validation accuracy is computed in the \"unfair\" manner using a single run of CRAFT over the full context preceding the actual personal attack, rather than the more realistic (and complicated) iterated evaluation that is used for final evaluation of the test set (in practice the two metrics track each other fairly well, making this a reasonable simplification for the sake of easy validation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eG1p_cH5fw9"
   },
   "source": [
    "## Part 5: define the evaluation procedure\n",
    "\n",
    "We're almost ready to run! The last component we need is some code to evaluate performance on the test set after fine-tuning is completed. This evaluation should use the full iterative procedure described in the paper, replicating how a system might be deployed in practice, without knowledge of where the personal attack occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode\n",
    "@torch.no_grad\n",
    "def evaluateDataset(dataset, finetuned_model, device, threshold=0.5, temperature=1.0):\n",
    "    finetuned_model = finetuned_model.to(device)\n",
    "    convo_ids = []\n",
    "    preds = []\n",
    "    scores = []\n",
    "    for data in tqdm(dataset):\n",
    "        input_ids = data['input_ids'].to(device, dtype = torch.long).reshape([1,-1])\n",
    "        attention_mask = data['attention_mask'].to(device, dtype = torch.long).reshape([1,-1])\n",
    "        outputs = finetuned_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probs = F.softmax(outputs.logits / temperature, dim=-1)\n",
    "        convo_ids.append(data[\"id\"])\n",
    "        raw_score = probs[0,1].item()\n",
    "        preds.append(int(raw_score > threshold))\n",
    "        scores.append(raw_score)\n",
    "    return pd.DataFrame({\"prediction\": preds, \"score\": scores}, index=convo_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0VIv8sbOaB6"
   },
   "source": [
    "## Part 6: build and fine-tune the model\n",
    "\n",
    "We finally have all the components we need! Now we can instantiate the CRAFT model components, load the pre-trained weights, and run fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=2)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    cls_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return cls_metrics.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/reef/conda-envs/jacq-zissou-env-3.11/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='395' max='395' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [395/395 02:34, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.727800</td>\n",
       "      <td>0.661939</td>\n",
       "      <td>0.610714</td>\n",
       "      <td>0.598773</td>\n",
       "      <td>0.617722</td>\n",
       "      <td>0.580952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.710400</td>\n",
       "      <td>0.649428</td>\n",
       "      <td>0.627381</td>\n",
       "      <td>0.603295</td>\n",
       "      <td>0.644986</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.582600</td>\n",
       "      <td>0.653353</td>\n",
       "      <td>0.626190</td>\n",
       "      <td>0.605528</td>\n",
       "      <td>0.640957</td>\n",
       "      <td>0.573810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.480100</td>\n",
       "      <td>0.661596</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.613527</td>\n",
       "      <td>0.622549</td>\n",
       "      <td>0.604762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.343800</td>\n",
       "      <td>0.666015</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.608802</td>\n",
       "      <td>0.625628</td>\n",
       "      <td>0.592857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/reef/conda-envs/jacq-zissou-env-3.11/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/reef/conda-envs/jacq-zissou-env-3.11/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/reef/conda-envs/jacq-zissou-env-3.11/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/reef/conda-envs/jacq-zissou-env-3.11/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=395, training_loss=0.6029376553583748, metrics={'train_runtime': 157.1897, 'train_samples_per_second': 79.776, 'train_steps_per_second': 2.513, 'total_flos': 3299412634214400.0, 'train_loss': 0.6029376553583748, 'epoch': 5.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"/reef/BERTCRAFT/{corpus_name}\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=6.7e-6,  # https://arxiv.org/pdf/2110.05111.pdf\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    prediction_loss_only=False,\n",
    "    run_name=f\"bertcraft_{corpus_name}\",\n",
    "    logging_steps=1,\n",
    "    seed=4300,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"val\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6.5: Threshold tuning\n",
    "\n",
    "For CRAFT, we selected the decision threshold by iterating over possible thresholds on the validation set and picking the one that gives the highest validation accuracy. Since the optimal threshold may be different for BERT, we replicate this process with our trained BERT forecaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "if corpus_name == 'wikiconv':\n",
    "    checkpoint = \"237\"  # ep0: 79, ep1: 158, ep3: 237\n",
    "elif corpus_name == 'cmv':\n",
    "    checkpoint = \"387\"  # ep0: 129, ep1: 258, ep3: 387\n",
    "# for custom data, specify your own checkpoint!\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    f\"/reef/BERTCRAFT/{corpus_name}/checkpoint-{checkpoint}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4399/4399 [00:44<00:00, 98.48it/s]\n"
     ]
    }
   ],
   "source": [
    "val_scores = evaluateDataset(tokenized_dataset[\"val_for_tuning\"], finetuned_model, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51451175.46452.46444</th>\n",
       "      <td>1</td>\n",
       "      <td>0.647627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51451323.46486.46486</th>\n",
       "      <td>1</td>\n",
       "      <td>0.628599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44473697.13344.13334</th>\n",
       "      <td>1</td>\n",
       "      <td>0.689201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44474698.13311.13580</th>\n",
       "      <td>1</td>\n",
       "      <td>0.724396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44474912.13327.13327</th>\n",
       "      <td>1</td>\n",
       "      <td>0.763027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162936818.6494.6494</th>\n",
       "      <td>0</td>\n",
       "      <td>0.199806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162940314.6722.6722</th>\n",
       "      <td>0</td>\n",
       "      <td>0.214158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162941948.8249.8217</th>\n",
       "      <td>0</td>\n",
       "      <td>0.240742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162944257.8619.8619</th>\n",
       "      <td>0</td>\n",
       "      <td>0.317453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162944360.8797.8797</th>\n",
       "      <td>1</td>\n",
       "      <td>0.705716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4399 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      prediction     score\n",
       "51451175.46452.46444           1  0.647627\n",
       "51451323.46486.46486           1  0.628599\n",
       "44473697.13344.13334           1  0.689201\n",
       "44474698.13311.13580           1  0.724396\n",
       "44474912.13327.13327           1  0.763027\n",
       "...                          ...       ...\n",
       "162936818.6494.6494            0  0.199806\n",
       "162940314.6722.6722            0  0.214158\n",
       "162941948.8249.8217            0  0.240742\n",
       "162944257.8619.8619            0  0.317453\n",
       "162944360.8797.8797            1  0.705716\n",
       "\n",
       "[4399 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each CONVERSATION, whether or not it triggers will be effectively determined by what the highest score it ever got was\n",
    "highest_convo_scores = {c.id: -1 for c in corpus.iter_conversations(lambda convo: convo.meta['split']==\"val\")}\n",
    "for utt_id in val_scores.index:\n",
    "    parent_convo = corpus.get_utterance(utt_id).get_conversation()\n",
    "    utt_score = val_scores.loc[utt_id].score\n",
    "    if utt_score > highest_convo_scores[parent_convo.id]:\n",
    "        highest_convo_scores[parent_convo.id] = utt_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_convo_ids = [c.id for c in corpus.iter_conversations(lambda convo: convo.meta['split'] == 'val')]\n",
    "val_labels = np.asarray([int(corpus.get_conversation(c).meta[label_metadata]) for c in val_convo_ids])\n",
    "val_scores = np.asarray([highest_convo_scores[c] for c in val_convo_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use scikit learn to find candidate threshold cutoffs\n",
    "_, _, thresholds = roc_curve(val_labels, val_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold is 0.5079520344734192 with accuracy 0.6357142857142857\n"
     ]
    }
   ],
   "source": [
    "def acc_with_threshold(y_true, y_score, thresh):\n",
    "    y_pred = (y_score > thresh).astype(int)\n",
    "    return (y_pred == y_true).mean()\n",
    "\n",
    "accs = [acc_with_threshold(val_labels, val_scores, t) for t in thresholds]\n",
    "best_acc_idx = np.argmax(accs)\n",
    "print(\"Best threshold is\", thresholds[best_acc_idx], \"with accuracy\", accs[best_acc_idx])\n",
    "best_threshold = thresholds[best_acc_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Na5gjZGE-KA0"
   },
   "source": [
    "## Part 7: run test set evaluation\n",
    "\n",
    "Now that we have successfully fine-tuned the model, we run it on the test set so that we can evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4365/4365 [00:44<00:00, 98.21it/s]\n"
     ]
    }
   ],
   "source": [
    "forecasts_df = evaluateDataset(tokenized_dataset[\"test\"], finetuned_model, \"cuda\", threshold=best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>240149353.16340.16332</th>\n",
       "      <td>0</td>\n",
       "      <td>0.258606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240153963.16690.16690</th>\n",
       "      <td>0</td>\n",
       "      <td>0.203976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241131342.18781.18523</th>\n",
       "      <td>0</td>\n",
       "      <td>0.385003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241132175.18826.18826</th>\n",
       "      <td>0</td>\n",
       "      <td>0.481780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463673734.2170.2155</th>\n",
       "      <td>0</td>\n",
       "      <td>0.330241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132878058.154555.154555</th>\n",
       "      <td>0</td>\n",
       "      <td>0.293458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132882061.154637.154637</th>\n",
       "      <td>1</td>\n",
       "      <td>0.577495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132930318.154870.154870</th>\n",
       "      <td>1</td>\n",
       "      <td>0.749400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132958763.155449.155449</th>\n",
       "      <td>1</td>\n",
       "      <td>0.791371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133004162.155823.155685</th>\n",
       "      <td>1</td>\n",
       "      <td>0.815458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4365 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         prediction     score\n",
       "240149353.16340.16332             0  0.258606\n",
       "240153963.16690.16690             0  0.203976\n",
       "241131342.18781.18523             0  0.385003\n",
       "241132175.18826.18826             0  0.481780\n",
       "463673734.2170.2155               0  0.330241\n",
       "...                             ...       ...\n",
       "132878058.154555.154555           0  0.293458\n",
       "132882061.154637.154637           1  0.577495\n",
       "132930318.154870.154870           1  0.749400\n",
       "132958763.155449.155449           1  0.791371\n",
       "133004162.155823.155685           1  0.815458\n",
       "\n",
       "[4365 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecasts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_EMZ7-SKtEP"
   },
   "source": [
    "## Part 8: merge predictions back into corpus and evaluate\n",
    "\n",
    "Now that the hard part is done, all that is left to do is to evaluate the predictions. Since the predictions are in no particular order, we will first merge each prediction back into the source corpus, and then evaluate each conversation according to the order of utterances within that conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Vnjmtu-QLDVo"
   },
   "outputs": [],
   "source": [
    "# We will add a metadata entry to each test-set utterance signifying whether, at the time\n",
    "# that CRAFT saw the context *up to and including* that utterance, CRAFT forecasted the\n",
    "# conversation would derail. Note that in datasets where the actual toxic comment is\n",
    "# included (such as wikiconv), we explicitly do not show that comment to CRAFT (since\n",
    "# that would be cheating!), so that comment will not have an associated forecast.\n",
    "for convo in corpus.iter_conversations():\n",
    "    # only consider test set conversations (we did not make predictions for the other ones)\n",
    "    if convo.meta['split'] == \"test\":\n",
    "        for utt in convo.iter_utterances():\n",
    "            if utt.id in forecasts_df.index:\n",
    "                utt.meta['forecast_score'] = forecasts_df.loc[utt.id].score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "FYxW_AuWszqX",
    "outputId": "d478288b-55ae-4bc9-cb8c-f294a80f3174"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6261904761904762\n"
     ]
    }
   ],
   "source": [
    "# Finally, we can use the forecast-annotated corpus to compute the forecast accuracy.\n",
    "# Though we have an individual forecast per utterance, ground truth is at the conversation level:\n",
    "# either a conversation derails or it does not. Thus, forecast accuracy is computed as follows:\n",
    "#   - True positives are cases that actually derail, for which the model made at least one positive forecast ANYTIME prior to derailment\n",
    "#   - False positives are cases that don't derail but for which the model made at least one positive forecast\n",
    "#   - False negatives are cases that derail but for which the model made no positive forecasts prior to derailment\n",
    "#   - True negatives are cases that don't derail, for which the model made no positive forecasts\n",
    "# Note that in the included datasets (wikiconv and cmv), by construction, all forecasts we obtained are forecasts made prior to derailment\n",
    "# (since these datasets end right before or right at the toxic comment). This simplifies  the computation of forecast metrics as we now \n",
    "# do not need to explicitly consider when a forecast was made. But if you are using a custom dataset where conversations continue past\n",
    "# the toxic comment, you will need to take that into account when evaluating.\n",
    "\n",
    "conversational_forecasts_df = {\n",
    "    \"convo_id\": [],\n",
    "    \"label\": [],\n",
    "    \"score\": [],\n",
    "    \"prediction\": []\n",
    "}\n",
    "\n",
    "for convo in corpus.iter_conversations():\n",
    "    if convo.meta['split'] == \"test\":\n",
    "        conversational_forecasts_df['convo_id'].append(convo.id)\n",
    "        conversational_forecasts_df['label'].append(int(convo.meta[label_metadata]))\n",
    "        forecast_scores = [utt.meta['forecast_score'] for utt in convo.iter_utterances() if 'forecast_score' in utt.meta]\n",
    "        conversational_forecasts_df['score'] = np.max(forecast_scores)\n",
    "        conversational_forecasts_df['prediction'].append(int(np.max(forecast_scores) > best_threshold))\n",
    "\n",
    "conversational_forecasts_df = pd.DataFrame(conversational_forecasts_df).set_index(\"convo_id\")\n",
    "print((conversational_forecasts_df.label == conversational_forecasts_df.prediction).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "_49Yaz2FIo9S",
    "outputId": "d1ae7b87-6973-41a1-e7aa-806d57990c01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.6157, recall = 0.6714\n",
      "False positive rate = 0.41904761904761906\n",
      "F1 = 0.6423690205011389\n"
     ]
    }
   ],
   "source": [
    "# in addition to accuracy, we can also consider applying other metrics at the conversation level, such as precision/recall\n",
    "def get_pr_stats(preds, labels):\n",
    "    tp = ((labels==1)&(preds==1)).sum()\n",
    "    fp = ((labels==0)&(preds==1)).sum()\n",
    "    tn = ((labels==0)&(preds==0)).sum()\n",
    "    fn = ((labels==1)&(preds==0)).sum()\n",
    "    print(\"Precision = {0:.4f}, recall = {1:.4f}\".format(tp / (tp + fp), tp / (tp + fn)))\n",
    "    print(\"False positive rate =\", fp / (fp + tn))\n",
    "    print(\"F1 =\", 2 / (((tp + fp) / tp) + ((tp + fn) / tp)))\n",
    "\n",
    "get_pr_stats(conversational_forecasts_df.prediction, conversational_forecasts_df.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzBiI0dsW7WZ"
   },
   "source": [
    "## Part 9: model analysis: how early is early warning?\n",
    "\n",
    "The goal of CRAFT is to forecast outcomes in advance, but how far in advance does it typically make its prediction? Following the paper, we measure this in two ways: the number of *comments* between the first prediction and the actual derailment, and how much *elapsed time* that gap actually translates to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "8Qfvl9k8Xesh"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'forecast_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# now scan the utterances in order until we find the first derailment prediction (if any)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(utts)):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mutts\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforecast_score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m>\u001b[39m forecast_thresh:\n\u001b[1;32m     19\u001b[0m         comments_until_derail[convo\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m derail_idx \u001b[38;5;241m-\u001b[39m idx\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;66;03m#time_until_derail[convo.id] = utts[derail_idx].timestamp - utts[idx].timestamp\u001b[39;00m\n",
      "File \u001b[0;32m/reef/conda-envs/jacq-zissou-env-3.11/lib/python3.11/site-packages/convokit/model/convoKitMeta.py:37\u001b[0m, in \u001b[0;36mConvoKitMeta.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# in DB mode, metadata field mutation would not be updated. (ex. mutating dict/list metadata fields)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# we align MEM mode behavior and DB mode by making deepcopy of metadata fields, so mutation no longer\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# affect corpus metadata backend, but only acting on the copy of it.\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     immutable_types \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mcomplex\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mfrozenset\u001b[39m)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, immutable_types):\n",
      "File \u001b[0;32m/reef/conda-envs/jacq-zissou-env-3.11/lib/python3.11/site-packages/convokit/model/backendMapper.py:179\u001b[0m, in \u001b[0;36mMemMapper.get_data\u001b[0;34m(self, component_type, component_id, property_name, index)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collection[component_id]\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollection\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcomponent_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mproperty_name\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'forecast_score'"
     ]
    }
   ],
   "source": [
    "comments_until_derail = {} # store the \"number of comments until derailment\" metric for each conversation\n",
    "time_until_derail = {} # store the \"time until derailment\" metric for each conversation\n",
    "\n",
    "for convo in corpus.iter_conversations():\n",
    "    if convo.meta['split'] == \"test\" and convo.meta[label_metadata]:\n",
    "        # filter out the section header as usual\n",
    "        utts = [utt for utt in convo.iter_utterances() if not (corpus_name == 'wikiconv' and utt.meta['is_section_header'])]\n",
    "        if utt_label_metadata is not None:\n",
    "            # if utterances have individual toxicity labels, we assume that the last comment in the conversation\n",
    "            # is the one that is toxic (as in the case for wikiconv)\n",
    "            derail_idx = len(utts) - 1\n",
    "        else:\n",
    "            # otherwise, we assume that the toxic comment is not included and that derailment happens immediately\n",
    "            # after the last comment in the conversation\n",
    "            derail_idx = len(utts)\n",
    "        # now scan the utterances in order until we find the first derailment prediction (if any)\n",
    "        for idx in range(1, len(utts)):\n",
    "            if utts[idx].meta['forecast_score'] > best_threshold:\n",
    "                comments_until_derail[convo.id] = derail_idx - idx\n",
    "                #time_until_derail[convo.id] = utts[derail_idx].timestamp - utts[idx].timestamp\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "IFXn4LrMhJ8W",
    "outputId": "26611644-8c44-4380-8897-b0de6eece0cd"
   },
   "outputs": [],
   "source": [
    "# compute some quick statistics about the distribution of the \"number of comments until derailment\" metric\n",
    "comments_until_derail_vals = np.asarray(list(comments_until_derail.values()))\n",
    "print(np.min(comments_until_derail_vals), np.max(comments_until_derail_vals), np.median(comments_until_derail_vals), np.mean(comments_until_derail_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "7cTdzAuLhuHF",
    "outputId": "85df20c5-62e1-4d2c-8736-8432a65f55ac"
   },
   "outputs": [],
   "source": [
    "# compute some quick statistics about the distribution of the \"time until derailment\" metric\n",
    "# note that since timestamps are in seconds, we convert to hours by dividing by 3600, to make it more human readable\n",
    "#time_until_derail_vals = np.asarray(list(time_until_derail.values())) / 3600\n",
    "#print(np.min(time_until_derail_vals), np.max(time_until_derail_vals), np.median(time_until_derail_vals), np.mean(time_until_derail_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "_w3l6UxDiDAz",
    "outputId": "7de51fb0-dbb9-42f2-8d5f-18fcda28ab00"
   },
   "outputs": [],
   "source": [
    "# visualize the distribution of \"number of comments until derailment\" as a histogram (reproducing Figure 4 from the paper)\n",
    "plt.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "plt.rcParams['font.size'] = 24\n",
    "plt.hist(comments_until_derail_vals, bins=range(1, np.max(comments_until_derail_vals)), density=True)\n",
    "plt.xlim(1,10)\n",
    "plt.xticks(np.arange(1,10)+0.5, np.arange(1,10))\n",
    "plt.yticks(np.arange(0,0.25,0.05), np.arange(0,25,5))\n",
    "plt.xlabel(\"Number of comments elapsed\")\n",
    "plt.ylabel(\"% of conversations\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CRAFT fine-tuning demo using ConvoKit",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
