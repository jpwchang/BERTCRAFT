{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from convokit import Corpus, download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_name = \"cmv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_metadata = \"conversation_has_personal_attack\" if corpus_name == \"wikiconv\" else \"has_removed_comment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /home/jonathan/.convokit/downloads/conversations-gone-awry-cmv-corpus\n"
     ]
    }
   ],
   "source": [
    "if corpus_name == \"wikiconv\":\n",
    "    corpus = Corpus(filename=download(\"conversations-gone-awry-corpus\"))\n",
    "elif corpus_name == \"cmv\":\n",
    "    corpus = Corpus(filename=download(\"conversations-gone-awry-cmv-corpus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the probability of randomly predicting derailment as the utterance-level\n",
    "# class probability, given as (# utts followed by a toxic comment / # of utts)\n",
    "n_utt_adj = 1 if corpus_name == \"wikiconv\" else 0 # subtract 2 from convo length in wikiconv since the toxic comment and section header are included but not counted\n",
    "n_utts = np.sum([len(convo.get_utterance_ids())-n_utt_adj for convo in corpus.iter_conversations(lambda c: c.meta['split'] == \"train\")])\n",
    "n_awry = len([c for c in corpus.iter_conversations(lambda c: (c.meta['split'] == \"train\" and c.meta[label_metadata]))]) # 1 toxic utt per awry convo\n",
    "p_awry = n_awry / n_utts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07931234305582384"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_awry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2024)\n",
    "for convo in corpus.iter_conversations():\n",
    "    # only consider test set conversations (we did not make predictions for the other ones)\n",
    "    if convo.meta['split'] == \"test\":\n",
    "        for utt in convo.iter_utterances():\n",
    "            # in wikiconv, skip section header and actual toxic comment\n",
    "            if corpus_name == \"wikiconv\" and (utt.meta['is_section_header'] or utt.meta['comment_has_personal_attack']):\n",
    "                continue\n",
    "            coin = random.random()\n",
    "            utt.meta[\"forecast\"] = int(coin < p_awry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.506578947368421\n"
     ]
    }
   ],
   "source": [
    "conversational_forecasts_df = {\n",
    "    \"convo_id\": [],\n",
    "    \"label\": [],\n",
    "    \"prediction\": []\n",
    "}\n",
    "\n",
    "for convo in corpus.iter_conversations():\n",
    "    if convo.meta['split'] == \"test\":\n",
    "        conversational_forecasts_df['convo_id'].append(convo.id)\n",
    "        conversational_forecasts_df['label'].append(int(convo.meta[label_metadata]))\n",
    "        conversational_forecasts_df['prediction'].append(np.max([u.meta.get('forecast', -1) for u in convo.iter_utterances()]))\n",
    "\n",
    "conversational_forecasts_df = pd.DataFrame(conversational_forecasts_df).set_index(\"convo_id\")\n",
    "print((conversational_forecasts_df.label == conversational_forecasts_df.prediction).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.5093, recall = 0.3596\n",
      "False positive rate = 0.34649122807017546\n",
      "F1 = 0.42159383033419023\n"
     ]
    }
   ],
   "source": [
    "# in addition to accuracy, we can also consider applying other metrics at the conversation level, such as precision/recall\n",
    "def get_pr_stats(preds, labels):\n",
    "    tp = ((labels==1)&(preds==1)).sum()\n",
    "    fp = ((labels==0)&(preds==1)).sum()\n",
    "    tn = ((labels==0)&(preds==0)).sum()\n",
    "    fn = ((labels==1)&(preds==0)).sum()\n",
    "    print(\"Precision = {0:.4f}, recall = {1:.4f}\".format(tp / (tp + fp), tp / (tp + fn)))\n",
    "    print(\"False positive rate =\", fp / (fp + tn))\n",
    "    print(\"F1 =\", 2 / (((tp + fp) / tp) + ((tp + fn) / tp)))\n",
    "\n",
    "get_pr_stats(conversational_forecasts_df.prediction, conversational_forecasts_df.label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jacq-zissou-env-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
